---
title: "Practical Machine Learning Assignment"
author: "Noman Khalid"
date: "Monday, April 27, 2015"
output: html_document
---

#Introduction
Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement Â- a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks.
One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this data set, the participants were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset).
In this project, the goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants toto predict the manner in which praticipants did the exercise.
The dependent variable or response is the "classe" variable in the training set. 

##Data

###Load the Data
```{r}
trainingOrg = read.csv("pml-training.csv", na.strings=c("", "NA", "NULL"))
testingOrg = read.csv("pml-testing.csv", na.strings=c("", "NA", "NULL"))
dim(trainingOrg)
dim(testingOrg)
```

###Pre-Screening the Data

There are several approaches for reducing the number of predictors.

-Remove variables that we believe have too many NA values.
```{r}
training.dena <- trainingOrg[ , colSums(is.na(trainingOrg)) == 0]
dim(training.dena)
training3 <- training.dena[ rowSums(is.na(training.dena)) == 0, ]
dim(training3)
```

-Remove unrelevant variables There are some unrelevant variables that can be removed as they are unlikely to be related to dependent variable.
```{r}
remove = c('X', 'user_name', 'raw_timestamp_part_1', 'raw_timestamp_part_2', 'cvtd_timestamp', 'new_window', 'num_window')
training.dere <- training.dena[, -which(names(training.dena) %in% remove)]
dim(training.dere)
```

-Check the variables that have extremely low variance (this method is useful nearZeroVar() )

```{r}
library(caret)
zeroVar= nearZeroVar(training.dere[sapply(training.dere, is.numeric)], saveMetrics = TRUE)
training.nonzerovar = training.dere[,zeroVar[, 'nzv']==0]
dim(training.nonzerovar)
```

-Remove highly correlated variables 90% (using for example findCorrelation() )
```{r}
corrMatrix <- cor(na.omit(training.nonzerovar[sapply(training.nonzerovar, is.numeric)]))
dim(corrMatrix)

corrDF <- expand.grid(row = 1:52, col = 1:52)
corrDF$correlation <- as.vector(corrMatrix)
levelplot(correlation ~ row+ col, corrDF)
```

We are going to remove those variable which have high correlation.
```{r,results='hide'}
removecor = findCorrelation(corrMatrix, cutoff = .90, verbose = TRUE)
training.decor = training.nonzerovar[,-removecor]
dim(training.decor)
```
*We get 19622 samples and 46 variables.*

##Splitting data into training and testing sets
```{r}
inTrain <- createDataPartition(y=training.decor$classe, p=0.7, list=FALSE)
training <- training.decor[inTrain,]
testing <- training.decor[-inTrain,]
dim(training)
dim(testing)
```
*We got 13737 samples and 46 variables for training, 5885 samples and 46 variables for testing.*

##Analysis

###Regression Tree
Now we fit a tree to these data, and summarize and plot it. First, we use the 'tree' package. It is much faster than 'caret' package.
```{r}
library(tree)
set.seed(12345)
tree.training=tree(classe~.,data=training)
summary(tree.training)

plot(tree.training)
text(tree.training,pretty=0, cex =.8)
```
*We need to trim this tree.*

###Rpart form Caret
```{r}
library(caret)
modFit <- train(classe ~ .,method="rpart",data=training)
print(modFit$finalModel)
```

*Using Rattle for Graph*
```{r}
library(rattle)
library(rpart.plot)
fancyRpartPlot(modFit$finalModel)
```
*The result from caret rpart package is close to tree package.*

##Cross Validation

We are going to check the performance of the tree on the testing data by cross validation.
```{r}
tree.pred=predict(tree.training,testing,type="class")
predMatrix = with(testing,table(tree.pred,classe))
sum(diag(predMatrix))/sum(as.vector(predMatrix)) # error rate
```
*The 0.72 is not very accurate.*
```{r}
tree.pred=predict(modFit,testing)
predMatrix = with(testing,table(tree.pred,classe))
sum(diag(predMatrix))/sum(as.vector(predMatrix)) # error rate
```
*0.50 from Caret Package is much less than the result from Decision Tree.*

###Modifying Tree

This tree was grown to full depth, and might be too variable. We now use Cross Validation to prune it.
```{r}
cv.training=cv.tree(tree.training,FUN=prune.misclass)
cv.training
plot(cv.training)
```
**It shows that when the size of the tree goes down, the deviance goes up. It means the 21 is a good size (i.e. number of terminal nodes) for this tree. We do not need to prune it.**

Suppose we prune it at size of nodes at 15.
```{r}
prune.training=prune.misclass(tree.training,best=15)
```
Now lets evaluate this pruned tree on the test data.
```{r}
tree.pred=predict(prune.training,testing,type="class")
predMatrix = with(testing,table(tree.pred,classe))
sum(diag(predMatrix))/sum(as.vector(predMatrix))
```
**0.64 is less than 0.72, so pruning did not hurt us with repect to misclassification errors, and gave us a simpler tree. We use less predictors to get almost the same result. By pruning, we got a shallower tree, which is easier to interpret.**

*The single tree is not good enough, so we are going to use bootstrap to improve the accuracy. We are going to try random forests.*

##Random Forests

Random forests build several trees, and then average them to reduce the variance.
```{r}
require(randomForest)
set.seed(12345)
```
*Testing a random forest*
```{r}
rf.training=randomForest(classe~.,data=training,ntree=100, importance=TRUE)
rf.training
varImpPlot(rf.training,)
```
**We can see which variables have higher impact on the prediction.**

###Out-of Sample Accuracy

Our Random Forest model shows OOB estimate of error rate: 0.79% for the training data. Now we will predict it for out-of sample accuracy.

Now lets evaluate this tree on the test data.
```{r}
tree.pred=predict(rf.training,testing,type="class")
predMatrix = with(testing,table(tree.pred,classe))
sum(diag(predMatrix))/sum(as.vector(predMatrix))
```
*0.99 means we got a very accurate estimate.*

No. of variables tried at each split: 6. It means every time we only randomly use 6 predictors to grow the tree. Since p = 43, we can have it from 1 to 43, but it seems 6 is enough to get the good result.

#Conclusion

Now we can predict the testing data from the website.
```{r}
answers <- predict(rf.training, testingOrg)
answers
```


*Answer File Writing Code*
```{r}
#pml_write_files = function(x){
#        n = length(x)
#        for(i in 1:n){
#                filename = paste0("problem_id_",i,".txt")
#                write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
#        }
#}

#pml_write_files(answers)
```